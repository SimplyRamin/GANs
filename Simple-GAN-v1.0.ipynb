{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest GAN with PyTorch!\n",
    "\n",
    "In this notebook we are going to implement simplest GAN possible! The chosen library for this task is PyTorch. A misunderstanding regarding the GANs are a false belief that they can only produce pictures. this is completely wrong, they can produce anything! in this notebook we are going to train a GAN which will produce odd numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions and creating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our first task is to make number binary and save each digit as list. with this approach we can check whether if a number is even or odd by looking at its right digit. also we can pass binary vectors to neural nets without difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary_list(number):\n",
    "    if type(number) is not int or number < 0:\n",
    "        raise ValueError('Enter Positive Integer!')\n",
    "    \n",
    "    return [int(x) for x in list(bin(number))[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets implement another function to generate odd number for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_odd_number(max_int, batch_size = 16):\n",
    "    max_length = math.ceil(math.log(max_int, 2))                # calculating the maximum length of possible numbers in binary\n",
    "    samples = []\n",
    "    while len(samples) != batch_size:\n",
    "        num = np.random.randint(0, max_int)\n",
    "        samples.append(num) if num % 2 != 0 else next           # adding number to list if that number is odd\n",
    "    labels = [1] * batch_size                                   # generating 1 as label for every number, 1 since all of them are odd already.\n",
    "    data = [convert_binary_list(x) for x in samples]            # converting generated odd numbers to binary\n",
    "    data = [([0] * (max_length - len(x))) + x for x in data]    # padding all of the number to same length by adding 0 to right digits\n",
    "\n",
    "    return labels, data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_float_matrix_to_int_list(float_matrix, threshold = .5):\n",
    "    return [int(\"\".join([str(int(y)) for y in x]), 2) for x in float_matrix >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "now lets make the generator! Since our task is fairly easy, a simple linear layer with sigmoid activation can do the job and there are no need to make this neural net sophisticated. note that for more complicated tasks, like generating sceneries, we might have to use more complicated neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(int(input_len), int(input_len))\n",
    "        self.activation1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.activation1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "the same thing goes for the discriminator too, it's not more complicate than generator. this part takes a binary number as input and checks whether that number is \"original\" (odd) or \"fake\" (even). since this task is fairly simple and easy, a linear layer with sigmoid activation can do this operation so there is no obligation to develope more complicated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear1 = nn.Linear(int(input_len), 1)\n",
    "        self.activation1 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.activation1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN\n",
    "i believe training is the trickest part of GAN. in this section we have too link the generator and discrimator and train them in unison. the reason behind this linkeage is correct propagating the gradients so the generator can \"learn\".\n",
    "\n",
    "at every training step in GAN we need 2 batches of data, one is random noise for generator to create a new number and second batch is our \"original\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_int = 128, batch_size = 16, epoch = 1000, print_output_n_steps = 10):\n",
    "    input_length = math.ceil(math.log(max_int, 2))\n",
    "\n",
    "    generator = Generator(input_length)\n",
    "    discriminator = Discriminator(input_length)\n",
    "\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr = .001)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = .001)\n",
    "\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(epoch):\n",
    "        generator_optimizer.zero_grad()                                                                                 # we have to zero gradients for each iteration.\n",
    "\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()                                            # creating noise for generator, Have to be Float, not int.\n",
    "        generated_data = generator(noise)\n",
    "\n",
    "        org_label, org_data = generate_odd_number(max_int, batch_size)                                                  # creating original data\n",
    "        org_label = torch.tensor(org_label).float()\n",
    "        org_label = org_label.unsqueeze(1)                                                                              # torch.size(16) and torch.size([16, 1]) are no longer equal\n",
    "        org_data = torch.tensor(org_data).float()\n",
    "\n",
    "        generator_discriminator_out = discriminator(generated_data)                                                     # training generator\n",
    "        generator_loss = loss(generator_discriminator_out, org_label)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        discriminator_optimizer.zero_grad()                                                                             # training discriminator\n",
    "        org_discriminator_out = discriminator(org_data)\n",
    "        org_discriminator_loss = loss(org_discriminator_out, org_label)\n",
    "\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())                                            # dont forget to detach\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size).unsqueeze(1))\n",
    "        discriminator_loss = (org_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        int_generated_data = convert_float_matrix_to_int_list(generated_data)                                           # converting generated data to int and calculating accuracy\n",
    "        generated_data_even_count = len([num for num in int_generated_data if num % 2 == 0])\n",
    "        error = (generated_data_even_count / batch_size) * 100\n",
    "\n",
    "        if i % print_output_n_steps == 0:\n",
    "            print(f'Step: {i}/{epoch}, Error: {error:.1f}%, Generator Loss: {generator_loss.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}, Overall loss: {generator_discriminator_loss.item():.4f}')\n",
    "            print(f'Sample: \\t{int_generated_data} \\n')\n",
    "\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/1001, Error: 50.0%, Generator Loss: 0.8213, Discriminator Loss: 0.7348, Overall loss: 0.5798\n",
      "Sample: \t[6, 85, 31, 94, 84, 95, 91, 95, 84, 31, 86, 15, 15, 92, 90, 92] \n",
      "\n",
      "Step: 100/1001, Error: 93.8%, Generator Loss: 0.6721, Discriminator Loss: 0.7372, Overall loss: 0.7150\n",
      "Sample: \t[68, 116, 124, 70, 76, 76, 68, 110, 85, 116, 68, 68, 68, 100, 108, 108] \n",
      "\n",
      "Step: 200/1001, Error: 100.0%, Generator Loss: 0.6647, Discriminator Loss: 0.7052, Overall loss: 0.7225\n",
      "Sample: \t[100, 100, 102, 102, 100, 110, 102, 100, 100, 100, 100, 100, 102, 102, 100, 116] \n",
      "\n",
      "Step: 300/1001, Error: 93.8%, Generator Loss: 0.7031, Discriminator Loss: 0.6849, Overall loss: 0.6834\n",
      "Sample: \t[100, 102, 102, 116, 100, 102, 102, 102, 102, 101, 102, 102, 102, 102, 114, 102] \n",
      "\n",
      "Step: 400/1001, Error: 93.8%, Generator Loss: 0.7133, Discriminator Loss: 0.6585, Overall loss: 0.6735\n",
      "Sample: \t[68, 66, 82, 70, 70, 80, 70, 70, 70, 7, 70, 68, 98, 70, 82, 70] \n",
      "\n",
      "Step: 500/1001, Error: 0.0%, Generator Loss: 0.6859, Discriminator Loss: 0.6687, Overall loss: 0.7005\n",
      "Sample: \t[81, 81, 81, 81, 65, 89, 65, 81, 65, 83, 81, 83, 83, 81, 81, 81] \n",
      "\n",
      "Step: 600/1001, Error: 0.0%, Generator Loss: 0.6688, Discriminator Loss: 0.6829, Overall loss: 0.7182\n",
      "Sample: \t[81, 9, 81, 121, 121, 57, 89, 17, 25, 25, 33, 33, 25, 17, 25, 113] \n",
      "\n",
      "Step: 700/1001, Error: 0.0%, Generator Loss: 0.6800, Discriminator Loss: 0.6773, Overall loss: 0.7066\n",
      "Sample: \t[57, 57, 57, 57, 57, 57, 49, 57, 57, 49, 57, 57, 57, 49, 49, 49] \n",
      "\n",
      "Step: 800/1001, Error: 0.0%, Generator Loss: 0.6998, Discriminator Loss: 0.6686, Overall loss: 0.6871\n",
      "Sample: \t[43, 41, 59, 59, 59, 11, 27, 43, 43, 59, 59, 43, 59, 49, 49, 57] \n",
      "\n",
      "Step: 900/1001, Error: 0.0%, Generator Loss: 0.6865, Discriminator Loss: 0.6834, Overall loss: 0.7002\n",
      "Sample: \t[15, 15, 15, 27, 15, 15, 15, 15, 15, 15, 11, 43, 15, 11, 11, 11] \n",
      "\n",
      "Step: 1000/1001, Error: 0.0%, Generator Loss: 0.6964, Discriminator Loss: 0.6770, Overall loss: 0.6900\n",
      "Sample: \t[7, 15, 31, 15, 7, 7, 15, 7, 7, 7, 7, 7, 15, 15, 15, 7] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Generator(\n",
       "   (linear1): Linear(in_features=7, out_features=7, bias=True)\n",
       "   (activation1): Sigmoid()\n",
       " ),\n",
       " Discriminator(\n",
       "   (linear1): Linear(in_features=7, out_features=1, bias=True)\n",
       "   (activation1): Sigmoid()\n",
       " ))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(epoch=1001, print_output_n_steps=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# By Ramin F. | @SimplyRamin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d1a71861c61e574d1e6f521fec382a1982cd4725da29f0a5c14efb87b3124eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
